## WH Multilayer Perceptron

Es gibt einen Input Layer, wo die Informationsquelle codiert eingelesen wird. In den Hidden Layers in der Mitte wird durch gewichtete Kanten auf die Knoten in den hidden layers per Knoten der resultierende Wert berechnet.

Der output layer heißt so, weil er der Layer ist, wo der Output abgelesen wird.

**Aktivierungsfunktionen:** Die moderne Aktivierungsfunktion ist a(x) = max(0, x) (=> 0 falls negativ und x falls positiv).

Die Werte des Input layers werden in einen Vektor geschrieben, die Gewichtung der Kanten auf Knoten des nächsten Layers wird in einer Matrix gespeichert. Dann wird multipliziert - das Resultat ist wieder ein Vektor, diesmal mit Dimension der Anzahl Knoten im zweiten Layer. Die Resultate werden noch mittels der Aktivierungsfunktion manipuliert (negative Werte werden nullgesetzt).

Net Input netj = Summe wij * xi

h1 = f(netj)

# Trainieren

Die Fehlerfunktion (*error function* oder *loss function*) ist folgendermaßen: ½(y - ŷ)²

(ŷ ist der aktuelle Wert und y ist der wahre Wert)

Der Fehler soll natürlich minimiert werden. Der Fehler lässt sich mit der quadratischen Funktion darstellen, wir wollen das Minimum des Fehlers finden.

Der Gradient ist von allen Werten abhängig (z.b. GradientL(w1,w2,w3) = (dL/dw1 dL/dw2 dL/dw3))

Es ist nicht einfach, den Fehler abhängig von **allen** Variablen zu bestimmen. Können Millionen an Gewichte sein.

* **Phase 1:** Forward propagation durch das Netzwerk  
* **Phase 2:** Back propagation of error

Wir nehmen die Ableitung jedes Pfades von einem Knoten auf einen anderen. Durch Ableitung erhält man den Einfluss, den jeder Pfad auf die Gewichtung des Endergebnisses hat.

netj = Summe(wiji\*oi + thetai)

Backpropagation des Fehlers. Error ist Differenz zwischen Target und Output jedes Neurons.

E = Summe(p)Summe(j)(tj^p - oj^p)^2

p.....Index des patterns
j.....Output Neuron
tj....Zielwert
oj....Aktueller Output

Zuerst Summe über alle Outputs, dann Summe über alle Daten, die das Netz sieht (z.B. bei 10000 Bildern hat man 10000 Werte für p).

ŷ ist o

## Wie sollen diese Gewichte verändert werden?

Die alten Gewichte werden verändert, damit man die neuen Werte bekommt.

wneu = walt + ∆w

Es wird rückwärts, ausgehend von den Kanten zum output layer, gewichtet.

loss function L(w) ist abhängig von allen Gewichten. Man will den Einfluss der Gewichte berechnen. Die Schwierigkeit darin ist, dass es mehrere layers gibt.

In unserem Bsp mit einem input/hidden/output layer mit 3/4/2 Knoten:

Um den Einfluss von wk1 (auf y1) zu bestimmen, muss man dL/dwk1 berechnen. (wk1 ist das Gewicht der Kante)

L(w) = 1/2(y - ŷ(w))^2

L'(w) = (ŷ(w) - y) * dŷ(w)/dwk1

= (ŷ(w) - y) \* dŷ(w)/dnetj \* dnetj/dwk1

**Wichtig zu merken:**

Vorwärtspropagation: Gewichte Vektor * Matrix => Werte für nächste Schicht. Fehlerfunktion muss definiert werden: 1/2\*(Wahrer Wert - aktueller Wert)^2 

Wir wollen dann das Minimum der Fehlerfunktion finden mit greatest descent Methode. Sobald man das hat, kann man den Einfluss jedes Gewichtes wissen und die Gewichte so verändern, dass das ganze System richtig arbeitet (korrekte Ergebnisse liefert).

