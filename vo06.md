## WH lineare Algebra, mehrdim. Diff.rechnung

Vektor^T × Matrix => Vektor
Matrix × Vektor   => Vektor

(1 2 3) × (1 2 3, 4 5 6)^T => (a,b)^T

nabla-f(x,y,z) => Gradientenvektor
nabla-f(f1,f2,f3) => Jacobimatrix

in welcher Richtung ist eine Steigung?

## cont. Neuronale Netze

Knoten, die vernetzt sind, werden modelliert. Die Komplexität ist beliebig groß, man fängt aber mit einer sehr einfachen Schicht an und wird immer komplexer und vernetzter.

Wieviele Ebenen und Neuronen pro Ebene man nutzt ist architekturabhängig.

## McCulloch-Pitts (MCP) Neuron

Das erste Modell eines Neurons wurde schon 1943 entwickelt. Man hat eine Funktion f, die von vielen Variablen abhängt. Sie liefert im Endeffekt eine Antwort (ein Wert), in dem Fall des ersten Modells 1/0.

f(x1,...,xn,y1,...,yn) = 0 oder 1.

McCulloch-Pitts Neuronen können die Boole'sche Algebra komplett nachbilden.

## Perceptron

Single Layer Perceptron (F. Rosenblatt 1957)

Erste Architektur, mit der man schon Probleme lösen konnte. Muster voneinander Trennen und Klassifizieren war möglich. Zwei Klassen von Objekten, eine Funktion um die beiden Cluster an Klassen voneinander zu trennen. 

f(x) = { 1 if w * x + b > 0, 0 otherwise}  
b = *bias*, Achsenabschnitt

w * x = Sigma wi * xi

quasi Vektor × Vektor (Vektor w^T × Vektor x = Wert)

Die Herausforderung ist, Gewichte w1 bis wn zu finden. Stichwort Gradientenvektor.

Stufenfunktionen: "Aktivierungsfunktionen"

Der Single Layer Perceptron kann nur lineare Patterns klassifizieren. Eine nicht-lineare Funktion kann man nicht damit modellieren.

* Kanten + Gewichte
* Knoten
* Aktivierungsfunktion f(x)

Eingabeschicht (erster Layer an Knoten, wo die Werte eingegeben werden)
Ausgabeschicht (letzer Layer, aus einem einzelnen Knoten bestehend, liefert Aktivierungsfunktion)

Aktivierungsfunktionen müssen keine Stufenfunktionen sein.

### XOR-Problem

```
1 0
0 1
```

kann nicht mit einer Linie voneinander getrennt werden.

## Aktivierungsfunktionen

* Threshold (step) function
* nonlineare Aktivierungsfunktionen

sigmold(x): 0 bei <0, bei x=0 ist 1/2 als Wendepunkt, bei >0 1.  
tanh(x): -1 bei <0, bei x=0 0 als Wendepunkt, 1 bei >0.
ReLU(x): max(0,x)

Lösung für das XOR-Problem:

## Multilayer Perceptron

Es gibt Schichten zwischen Eingabe- und Ausgabeschicht. Die Zwischenschichten führen zu komplett anderen Ergebnissen.

```
O   O   O     Input layer x=(x1,x2,x3)
| X | X |
O   O   O     Hidden layers
  V   V
  O   O       (Hidden layer 2)
  | X |
  O   O

Es sind alle Neuronen von angrenzenden Schichten miteinander verbunden.
(hier ist nur ein Teil verbunden)
```

Gewichtsmatrix:
3 Zeilen, 4 Spalten

Jetzt wird der Vektor des Inputs mit der Gewichtsmatrix multipliziert (Vektor transformieren). Es ergibt sich wieder ein Vektor mit neuen Dimensionen. Nächster Schritt ist das Anwenden der max(0,x)-Funktion. Dann werden die Werte mit der nächsten Gewichtsmatrix multipliziert.

Ergebnis insgesamt (output) ist ein Vektor mit Dimension der Anzahl der output nodes.

"Supervised Learning" das Beispiel von letzter Woche mit der Zahlenerkennung


